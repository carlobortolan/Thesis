% !TeX root = ../main.tex
% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Scaling TUMLive}\label{chapter:scaling_tumlive}

This chapter focuses on applying the concepts discussed in the previous sections to the architecture of TUMLive, discussing the development approach, results and challenges along the way. 

\section{Process, Preparation, Methods and Environments}

The thesis spanned 5 months, from 15.05.2024 to \getSubmissionDate{}. The first weeks were spent familiarizing with the current system considering different approaches to scale its architecture and finding potential bottlenecks or issues. Following the initial analysis of the system, before being able to try scaling individual components, the original user and role system first needed to be updated together with new database models. After that, the most important components were gradually updated to be usable and manageable by each faculty. 
To develop and test the prototype of a distributed architecture for TUM-Live for this thesis, the following resources were used:
\begin{itemize}
    \item 3 \ac{VM}s with: 2 GB RAM, Intel(R) Xeon(R) CPU E5-2697A v4 @ 2.60GHz
    \item 1 \ac{VM} with: 20 GB RAM, AMD EPYC 7452 32-Core Processor
    \item 1 \ac{AWS} \ac{EKS} Cluster
    \item 1 selfhosted \ac{VM} with 32 GB RAM, AMD Ryzen 7 PRO 6850U @ 2.70GHz
\end{itemize}

After the target architecture had been deployed (on a smaller scale) using given resources, a set of performance tests and comparisons was made to find potential limits and breakpoints of each component. 
Additionally, in parallel to the development of the new architecture, a dedicated documentation has been created to facilitate the set up of GoCast for lecturers or new faculties which can be found at \href{https://tumlive-docs.pages.dev/}{tumlive-docs.pages.dev}. The documentation was created using Meta Opensource's Docusaurus. Static pages were deployed using Cloudflare.
All relevant source code for the thesis, new architecture, other mentioned prototypes and documentation can also be found at \href{https://github.com/carlobortolan/thesis}{github.com/carlobortolan/thesis}.

\section{Proposed System}

\subsection{Target System Architecture}

To distribute GoCast to different faculties, the subsystems and components responsible for processing and handling video data need to be distributed and hosted by each individual school. 
The main Tum-Live \ac{API} instance however, will remain managed by the \ac{ITO} or TUM so that users have a single point of access (instead of having to switch instances when wanting to watch lectures of different faculties).
To achieve this, each faculty needs to host at least three components: the Runner / Worker component, the VOD Service component and an Edge Server. 
Each school can then decide how many resources it wants to allocate to each service depending on their expected usage. The following minimum hardware requirements are set:

\begin{itemize}
    \item At least 1 \ac{VM} as an Edge server. This server serves the videos to the users. Network throughput is important, so if a school serves many users, more instances are needed.
    \item At least 1 Worker or Runner \ac{VM}. This server produces the stream, transcodes the \ac{VOD} and much more. CPU performance is important here. On the same node, for every worker a VOD Service needs to be deployed to expose a simple HTTP interface that accepts file uploads and packages them to a \ac{HLS} stream in a configured location. This stream may then be distributed by the Edge Server.
    \item Optionally, a school can add additional \ac{VM}s for monitoring (Grafana, Prometheus, Loki, etc.) or for deploying services such as the Voice Service for subtitling live streams and \ac{VOD}s using the Whisper LLM.
\end{itemize}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=390pt]{images/NewDeploymentDiagram.png}
    \caption[Target System Architecture]{Target Deployment Diagram of TUM-Live}\label{fig:system-architecture}
\end{figure}

\newpage


\section{Role Based Access Control}\label{section:rbac}

Given the proposed system design, this section documents how this system has been implemented and what the current limitations and potential improvements of the main user system are. 

\subsection{Updating Architecture and Core API}

Initially, GoCast was used primarily by the former faculty of Informatics at TUM. However, with increasing demand, GoCast needs to be extended for university-wide lecture streaming. The solution to this are schools (not to be confused with TUMOnline's 'TUMOnline School').

The first step of implementing this structure was updating GoCast's role system. Previously, it contained four roles: \texttt{admin}, \texttt{lecturer}, \texttt{student}, \texttt{visitor}. While users with the admin role can manage over the entire system (e.g., create and delete lectures and users, as well as perform maintenance tasks), lecturers can only manage their own lectures and students only manage their own profile and preferences. However, with the introduction of schools, this is not sufficient, as each school needs to have own school-scoped admins that can manage over their schools' resources without being able to interfere with other schools. To do this, a new \texttt{maintainer}-role has been introduced (see \autoref{fig:school-hierarchy}).
With this, a school using GoCast is managed by a set of maintainers of this school. A user with the maintainer role can be maintainer of multiple schools, and has also maintainer rights for all sub-schools of his schools.  
Maintainers also have some basic administrative functionality which is limited to their schools' scope (e.g., create, update and delete courses and streams only for those schools which are administered by that maintainer). 

\subsection{TUMOnline School vs. GoCast School}

TUMOnline has a strict hierarchical structure for its organizations (one school has multiple departments; one department has multiple chairs; one chair has multiple courses ...).
%
% On a side node, TUMOnline has 7 schools, 29 departments and 487 chairs.
While GoCast is mainly used by the TUM, in principle it doesn't need to differentiate between organizational types that strictly. Organizations are only relevant when it comes to distributing the livestreams and recordings of a certain entity to that entity's resources (e.g., Workers/Runners and VOD Services). Hence, the introduction of GoCast's "schools" which represent an entity responsible for processing data. In practice, this is most of the time a TUMOnline school, however, one can also create a GoCast "school" for a department, chair or smaller organization which is subordinated to another organization, depending on the specific situation.

Here's an example to illustrate this in a more detailed way:
The TUMOnline "School of Management" (SOM) wants to start using GoCast. Hence, the SOM's IT team contacts the admins of GoCast who then create a new GoCast "school" TUM School and assign the SOM IT team as maintainers.
The subordinated "Chair of Financial Management and Capital Markets" (FA), however, has its own data center and wants to host its lectures with its own resources. In this case, either one of the SOM maintainers or the RBG can create a new GoCast "school" as a child of the SOM school and accordingly assign new maintainers from the FA-team. Now, the the FA-maintainers have full control over their sub-school and can connect their own resources from their data center with GoCast, independently of the SOM.

% The idea is the following: To avoid one entity having to manage and process all streaming data for the entire university (or multiple universities), GoCast is distributed to multiple entities. Each entity (aka GoCast 'school') has so-called maintainers (users with the maintainer user role) that are allowed to manage the school's resources such as Workers/Runners, VOD Services, etc.


% One maintainer can maintain multiple schools.
% The following school-related actions are allowed by a maintainer of a school:

%     Create, update or delete school

%     Create new tokens for that school (required to add new resources)

%     Manage school's resources

%     Manage school's maintainers




\section{Distributed Resources}

Now that the user role system had been updated, the next step was to find a solution to have the different resources such as Workers and Runners could be connected to the main cluster of Workers and Runners independent of the others. However, at the same time, they should be able to process and distribute requests between each other regardless of the school they are in. This section explains in detail how each subsystem works as part of the distributed GoCast system. 

\subsection{Workers and Runners}
To set up a distributed network of Workers and Runners, the first step was to update the system in such a way that Workers and Runners can be connected by a school's maintainer to the main network of GoCast. To do this, a school maintainer can create a new school-token, a \ac{JWT} that expires after seven hours and allows its owner to connect new resources for the school (see \autoref{fig:school-token}).

\begin{figure}[htpb]
    \centering
    \includegraphics[width=390pt]{images/SchoolToken.png}
    \caption[Example School Token]{Example School Token}\label{fig:school-token}
\end{figure}

When a maintainer starts a new Worker or Runner, he can include this token in the containers environment (e.g., via a \texttt{docker-compose.yml} file or by passing it as an argument directly to Docker using \texttt{-e Token=<...>}). When starting up, the Worker or Runner sends a requests to join the Worker or Runner pool of GoCast using the school-token and additional data (e.g., host name, IP or FQDN address, \ac{VM} workload, etc.) and - if successful - receives a token without expiration which is then used to identify all subsequent requests from and to the Worker or Runner.  

Maintainers can see and manage the current status of their registered resources in the Resource-Dashboard of GoCast (see \autoref{fig:resource-dashboard}).

\begin{figure}[htpb]
    \centering
    \includegraphics[width=390pt]{images/ResourceDashboard.png}
    \caption[Resources Dashboard]{Resources Dashboard}\label{fig:resource-dashboard}
\end{figure}

Now, whenever a new lecture is being recorded or uploaded, the main GoCast \ac{API} selects an available worker for this school and addresses it given the host name and IP address that the worker has registered when first connecting to GoCast. For more details on how the distribution of tasks works, see \autoref{section:shared-resources}.

When the Worker or Runner receives such a request, it initiates the video transcoding process using FFmpeg. The transcoding is performed based on the specific stream version (e.g., \texttt{CAM}, \texttt{PRES} or \texttt{COMB}), adjusting parameters like compression level and priority (niceness) accordingly. The process is monitored in real-time, with progress being reported back to the GoCast system, ensuring that any issues are retried with backoff strategies and that the final transcoded video meets the expected duration and quality standards.

\subsection{VOD Service and Edge Server}

Once the Worker or Runner has completed the transcoding process, the VOD Service starts detecting silent sections in the recordings so that the user will be able to skip these and get directly to the start of the actual lecture. Then, it packages them to a \ac{HLS} stream and pushes the packaged stream to the school's recording storage.

To ensure easy access to these recordings, each school also needs to have its own Edge Server set up. The VOD Service at each school is responsible for managing the processing, packaging, and storage of the recordings locally, while the Edge Server is responsible for distributing the streams. This also has the advantage that each school maintains control over its data, and reduces latency by storing the recordings closer to the end-users (assuming that a user is more likely to be closer to his school than to others).
Additionally, the Edge Server acts as a local cache for the school's content. When students access recordings, the Edge Server distributes the content directly from the local storage, reducing the load on the central servers.

The dependency on Workers or Runners for transcoding means that the efficiency and speed of the VOD Service and Edge Server setup depend on the availability and performance of these services. Therefore, it is necessary that a school has enough Workers or Runners to support the number of recorded lectures.
If a school has many concurrent viewers, it might need to consider having multiple Edge Servers running, as they have a limited bandwidth. %and can only serve approximately xxx.
When network traffic to worker nodes exceeds the available bandwidth, the architecture might look like the example shown in \autoref{fig:edge-network}.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\linewidth]{images/EdgeNetwork.png}
    \caption[Edge Server as proxy and cache node]{Edge Server as proxy and cache node}\label{fig:edge-network}
\end{figure}

% \subsection{Ingest Servers}
% TODO: How they are registered and distributed by the RTMP-Proxy
% \subsection{Voice Service}

\subsection{RTMP-Proxy}

GoCast supports not just recordings from the lecture hall or manual uploads, but also so called self-streams. The idea behind self-streams is, that any lecturer can go live at any given time and stream from his own device without having to be in the lecture hall. In past, to do this, a lecturer had to go open GoCast, go to one his courses' page, select a lecture, and copy a link (such as: \texttt{rtmp://worker.example.com:1935/cs-123? secret=5caa9d6447564cb5822995888e224f9a}) together with a secret stream key into a streaming software like OBS Studio. With the introduction of the school system there are several reasons why this can't work:

\begin{enumerate}
    \item The URL might change depending on the availability of workers and schools.
    \item Having to copy and paste a new URL every time a lecturer wants to start a stream can become annoying.
    \item For lecturers who teach at multiple schools (e.g., the Chair of Databases and the Chair of Bioinformatics), managing different URLs for each school would be inconvenient.
\end{enumerate}

The solution for this is the RTMP-Proxy microservice. In a nutshell, the RTMP-Proxy acts as a router-like service that accepts all \ac{RTMP} self-stream requests and redirects them accordingly. With this, all a lecturer has to is creating a new personal Token (see \autoref{fig:personal-token}) and copy the token together with the target URL of the RTPM-Proxy into a streaming software only once.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{images/PersonalToken.png}
    \caption[Creation of Personal Token and RTMP-Proxy URL]{Creation of Personal Token and RTMP-Proxy URL}\label{fig:personal-token}
\end{figure}

After this initial setup, the lecturer can go live whenever they want by simply using the pre-configured URL and token.

The internal process of the RTMP-Proxy is as follows: 

\begin{enumerate}
    \item An incoming \ac{RTMP} request is received: 
    \texttt{rtmp://rtmp-proxy.example.com:1935/ <user-id>?token=<personal-token>}.
    \item The RTMP-Proxy sends a self-stream request to the main GoCast \ac{API} with the provided \texttt{user-id} and \texttt{personal-token}.
    \item The GoCast \ac{API} returns the details of the school from which the user is streaming, including the \ac{RTMP} URL of the ingest worker currently available for that school.
    \item The RTMP-Proxy redirects the \ac{RTMP} request to the retrieved \ac{RTMP} URL.
    \item The school's ingest worker detects whether the lecturer has a scheduled lecture for the current time slot or an upcoming lecture and automatically starts the live-stream for viewers.
\end{enumerate}

While the RTMP-Proxy is far from perfect, it is meant as a prototype of a possible solution to stream via \ac{RTMP} in a distributed network where the final target address of the stream request is unknown when making the request. As the RTMP-Proxy is independent of other services in the GoCast environment (besides the main \ac{API}), it can be scaled very easily by deploying multiple RTMP-Proxies and having for example a Round-robin DNS distribute the load accordingly.

\section{Shared Resources}\label{section:shared-resources}

This section explains different approaches for the allocation of resources in the GoCast network. The main focus is on the task distribution to the available Workers and Runners when receiving new video upload requests.

\subsection{Shared Resources}

Before explaining how resources in the GoCast Network are dynamically allocated to different tasks, it is first important to explain in detail how a school actually "owns" a certain resource such as a Worker. As described in \autoref{section:rbac}, schools are organized in a hierarchical manner. One school can have multiple sub-schools which can again have multiple sub-sub-schools and so forth. When a school has deployed its own resources, by default accessible only by the school itself and its sub-schools. Using the example shown in \autoref{fig:school-hierarchy}, this would mean that if there's a lecture uploaded by \textit{School 1}, only public resources and the school's own private resources are considered. For a lecture of \textit{School 2a}, all public resources, the school's own private resources as well as its parent-school's resources (and recursively so on) are considered. 

\begin{figure}[htpb]
    \centering
    \includegraphics[width=250pt]{images/SchoolHierarchy.png}
    \caption[Example School Hierarchy]{Example School Hierarchy}\label{fig:school-hierarchy}
\end{figure}

Additionally, for certain resources, for example a Workers and Runners, one can decide to set the \texttt{Shared} flag. When a resource has that flag enabled, it means that even though it is part of a school (and hence by default only available for lectures of that school or sub-schools in that school's hierarchy), it now becomes available to all schools and shares its compute power with the entire network of schools. To use the previous example, assuming that \textit{School 2a} has allowed to shared its resources, when a \textit{School 1} uploads a video, in addition to its own private resources and the public resources, it also considers the shared resource of \textit{School 2a}.

\subsection{Four Approaches for Task Distribution}

The following are four approaches considered for the decision algorithm behind the allocation of resources - especially of Workers and Runners - in the GoCast Network. The current implementation for the task distribution of Workers uses the second approach. For the Runners, it remains to be seen which approach will be chosen - most likely a combination of the second and third approach. 

% \begin{enumerate}
    % \item \textbf{Randomized Allocation:}  
\subsubsection{1. Randomized Allocation}
     Tasks are assigned to available resources randomly. This method is simple and helps to distribute tasks across resources in an unbiased manner. However, it only considers a school's own and public resources, ignoring the hierarchical structure of schools or resource workloads. Randomized allocation can be effective if resource capabilities were relatively uniform, but it would most likely lead to inefficiencies if the networks got more complex.

    \begin{figure}[htpb]
      \begin{tabular}{c}
      \ \small \begin{lstlisting}[language=SQL]
        -- 1. Randomized allocation example query
        SELECT *
        FROM workers 
        WHERE shared = TRUE OR school_id = ?
        ORDER BY RANDOM()
        LIMIT 1;
        \end{lstlisting}
      \end{tabular}
      \label{fig:randomized-allocation}
    \end{figure}
    
    % \item \textbf{Recursive School-Based Allocation:}  
\subsubsection{2. Recursive School-Based Allocation}
    When a task (i.e. a video upload) is started, the system recursively checks for available resources, starting from the school's own resources and moving up the hierarchy to include parent schools and shared resources as described in the previous subsection. This ensures that each task is allocated to the most appropriate resource, improving resource utilization across the network by allowing sub-schools to benefit from the resources of their parent schools, while respecting the resource ownership rules defined by each school.

    \begin{figure}[htpb]
      \begin{tabular}{c}
      \ \small \begin{lstlisting}[language=SQL]
        -- 2. Recursive school-based allocation example query
        WITH RECURSIVE school_hierarchy AS (
            SELECT id, parent_id FROM schools WHERE id = ?
            UNION ALL
            SELECT s.id, s.parent_id FROM schools s
            INNER JOIN school_hierarchy sh ON s.id = sh.parent_id
        )
        SELECT w.* FROM workers w
        INNER JOIN school_hierarchy sh ON sh.id = w.school_id
        WHERE w.last_seen > NOW() - INTERVAL 5 MINUTE OR w.shared = true;
        \end{lstlisting}
      \end{tabular}
      \label{fig:recursive-allocation}
    \end{figure}

    % \item \textbf{Priority-Based Allocation:}  
\subsubsection{3. Priority-Based Allocation}
    Assign tasks to Workers and Runners based on a dynamically adjusted priority queue. The tasks would be prioritized based on factors such as viewer count, failure rate, \ac{VM} workload or the type of content being processed. For instance, live-streaming tasks could be given a higher priority over \ac{VOD} uploads and hence be processed by a Worker that runs on a more performant \ac{VM}. This approach would complete critical tasks first, and re-schedule less urgent tasks for a later point in time.

    \begin{figure}[htpb]
      \begin{tabular}{c}
      \ \small \begin{lstlisting}[language=Java]
        // 3. Priority-based allocation pseudocode
        function processTaskQueue(taskQueue) {
            while not taskQueue.isEmpty() {
                task = taskQueue.dequeue()
                worker := getNextAvailableWorker()
                
                if worker != nil {
                    worker.assignTask(task)
                } else {
                    taskQueue.enqueue(task) // Requeue task if no workers are available
                    wait(some-time)
                }
            }
        }
        \end{lstlisting}
      \end{tabular}
      \label{fig:priority-based-allocation}
    \end{figure}

\subsubsection{4. Linear Programming Optimization Approach}

By setting up and solving a \ac{LP}, the tasks can be distributed evenly among Workers in such a way that the maximum load on any single worker is minimized and bottlenecks are reduced.

\begin{enumerate}
    \item \textbf{Variables and Parameters}

Let:
\begin{itemize}
    \item \( n \) be the number of tasks.
    \item \( m \) be the number of Workers.
    \item \( t_i \) be the processing time of task \( i \).
    \item \( x_{ij} \) be a binary decision variable, where \( x_{ij} = 1 \) if task \( i \) is assigned to Worker \( j \), and \( x_{ij} = 0 \) otherwise.
    \item \( L_j \) be the total workload on Worker \( j \), calculated as \( L_j = \sum_{i=1}^{n} t_i \cdot x_{ij} \).
    % \item \( L_{\text{max}} \) be the maximum load across all Workers, defined as \( L_{\text{max}} = \max_{j=1}^{m} L_j \).
\end{itemize}

\item \textbf{Objective Function}

The objective is to minimize the maximum load \( L_{\mathrm{max}} \) across all Workers:
\begin{equation}
\operatorname{Minimize\ } L_{\text{max}}
\end{equation}

\item \textbf{Constraints}

The \ac{LP} is subject to the following constraints:

\begin{enumerate}
    \item \textbf{Task Assignment:} Each task \( i \) must be assigned to exactly one Worker:
    \begin{equation}
    \sum_{j=1}^{m} x_{ij} = 1 \quad \forall i = 1, \dots, n
    \end{equation}

    \item \textbf{Load Calculation:} The workload of each Worker \( j \) is the sum of the processing times of the tasks assigned to that Worker:
    \begin{equation}
    L_j = \sum_{i=1}^{n} t_i \cdot x_{ij} \quad \forall j = 1, \dots, m
    \end{equation}

    \item \textbf{Bounding the Maximum Load:} The workload on each Worker must be less than or equal to the maximum load \( L_{\text{max}} \):
    \begin{equation}
    L_j \leq L_{\text{max}} \quad \forall j = 1, \dots, m
    \end{equation}
    
    \item \textbf{Binary Variables:} The assignment variables \( x_{ij} \) are binary:
    \begin{equation}
    x_{ij} \in \{0, 1\} \quad \forall i = 1, \dots, n \quad \text{and} \quad \forall j = 1, \dots, m
    \end{equation}
\end{enumerate}

\item \textbf{Linear Programming Formulation:}

Given the objective function and the constraints, the solution to the \ac{LP} can be formulated as:
\begin{align}
\text{Minimize} \quad & L_{\text{max}} \\
\text{s.t.} \quad & \sum_{j=1}^{m} x_{ij} = 1 \quad \forall i = 1, \dots, n \\
& L_j = \sum_{i=1}^{n} t_i \cdot x_{ij} \quad \forall j = 1, \dots, m \\
& L_j \leq L_{\text{max}} \quad \forall j = 1, \dots, m \\
& x_{ij} \in \{0, 1\} \quad \forall i = 1, \dots, n \quad \text{and} \quad \forall j = 1, \dots, m
\end{align}
\item \textbf{Example Implementation:} 

    \begin{figure}[htpb]
      \begin{tabular}{c}
      \ \small \begin{lstlisting}[language=Python]
        # Using Gurobi as an example - any other solver also works
        from gurobipy import Model, GRB, quicksum
        model = Model("Minimize_Max_Load")
        
        # Decision variables: x[i, j] = 1 if task i is assigned to worker j, 0 otherwise
        x = model.addVars(n, m, vtype=GRB.BINARY, name="x")
        
        # Variable for maximum load across all workers
        L_max = model.addVar(vtype=GRB.CONTINUOUS, name="L_max")
        
        # Objective: Minimize the maximum load L_max
        model.setObjective(L_max, GRB.MINIMIZE)
        
        # Constraint 3.a) Each task is assigned to exactly one worker
        for i in range(n):
            model.addConstr(
                quicksum(x[i, j] for j in range(m)) == 1, name=f"Task_Assignment_{i}")
        
        # Constraint 3.b) and 3.c) Load on each worker does't exceed L_max
        for j in range(m):
            model.addConstr(
                quicksum(t[i] * x[i, j] for i in range(n)) <= L_max, 
                name=f"Worker_Load_{j}")
        
        # Run the model
        model.optimize()
        \end{lstlisting}
      \end{tabular}
      \label{fig:lp-optimization}
    \end{figure}

\end{enumerate}
