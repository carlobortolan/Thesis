% !TeX root = ../main.tex
% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Scaling TUMLive}\label{chapter:scaling_tumlive}

This chapter focuses on applying the concepts discussed in the previous sections to the architecture of TUMLive, discussing the development approach, results and challenges along the way. 

\section{Process, Preparation, Methods and Environments}

The thesis spanned 5 months, from 15.05.2024 to \getSubmissionDate{}. The first weeks were spent familiarizing with the current system considering different approaches to scale its architecture and finding potential bottlenecks or issues. Following the initial analysis of the system, before being able to try scaling individual components, the original user and role system first needed to be updated together with new database models. After that, the most important components were gradually updated to be usable and manageable by each faculty. 
To develop and test the prototype of a distributed architecture for TUM-Live for this thesis, the following resources were used:
\begin{itemize}
    \item 3 \ac{VM}s with: 2 GB RAM, Intel(R) Xeon(R) CPU E5-2697A v4 @ 2.60GHz
    \item 1 \ac{VM} with: 20 GB RAM, AMD EPYC 7452 32-Core Processor
    \item 1 \ac{AWS} \ac{EKS} Cluster
    \item 1 selfhosted \ac{VM} with 32 GB RAM, AMD Ryzen 7 PRO 6850U @ 2.70GHz
\end{itemize}

After the target architecture had been deployed (on a smaller scale) using given resources, a set of performance tests and comparisons was made to find potential limits and breakpoints of each component. 
Additionally, in parallel to the development of the new architecture, a dedicated documentation has been created to facilitate the set up of GoCast for lecturers or new faculties which can be found at \href{https://tumlive-docs.pages.dev/}{tumlive-docs.pages.dev}. The documentation was created using Meta Opensource's Docusaurus. Static pages were deployed using Cloudflare.
All relevant source code for the thesis, new architecture, other mentioned prototypes and documentation can also be found at \href{https://github.com/carlobortolan/thesis}{github.com/carlobortolan/thesis}.

\section{Proposed System}

\subsection{Target System Architecture}

To distribute GoCast to different faculties, the subsystems and components responsible for processing and handling video data need to be distributed and hosted by each individual school. 
The main Tum-Live API instance however, will remain managed by the \ac{ITO} or TUM so that users have a single point of access (instead of having to switch instances when wanting to watch lectures of different faculties).
To achieve this, each faculty needs to host at least three components: the Runner / Worker component, the VoD service component and a Edge Server. 
Each school can then decide how many resources it wants to allocate to each service depending on their expected usage. The following minimum hardware requirements are set:

\begin{itemize}
    \item At least 1 VM as an Edge server. This server serves the videos to the users. Network throughput is important, so if a school serves many users, more instances are needed.
    \item At least 1 Worker or Runner VM. This server produces the stream, transcodes the VoD and much more. CPU performance is important here. On the same node, for every worker a VoD Service needs to be deployed to expose a simple HTTP interface that accepts file uploads and packages them to a HLS stream in a configured location. This stream may then be distributed by the Edge Server.
    \item Optionally, a school can add additional VMs for monitoring (grafana, prometheus, influx...) or for services such as the Voice Service for subtitling live streams and VoDs which requiring a NVIDIA CUDA equipped Server to transcribes streams using the Whisper LLM).
\end{itemize}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=390pt]{images/NewDeploymentDiagram.png}
    \caption[Target System Architecture]{Target Deployment Diagram of TUM-Live}\label{fig:system-architecture}
\end{figure}

\newpage


\section{Role Based Access Control}\label{section:rbac}

Given the proposed system design, this section documents how this system has been implemented and what the current limitations and potential improvements of the main user system are. 

\subsection{Updating Architecture and Core API}

Initially, GoCast was used primarily by the former faculty of Informatics at TUM. However, with increasing demand, GoCast needs to be extended for university-wide lecture streaming. The solution to this are schools (not to be confused with TUMOnline's 'TUM School').

The first step of implementing this structure was updating GoCast's role system. Previously, it contained four roles: admin, lecturer, student, visitor. While users with the admin role can manage over the entire system (e.g., create and delete lectures and users, as well as perform maintanance tasks), lecturers can only manage their own lectures and students only manage their own profile and preferences. Howver, with the introduction of schools, this is not sufficient, as the each school needs to have own school-scoped admins that can manage over their schools' resources without being able to interfere with other schools. To do this, a new 'maintainer'-role has been introduced (see below graph).
With this, a school using GoCast is managed by a set of maintainers of this school. A user with the maintainer role can be maintainer of multiple schools, and has also maintainer rights for all sub-schools of his schools.  
Maintainers also have some basic administrative functionality which is limited to their schools' scope (e.g., create, update and delete courses and streams only for those schools which are administered by that maintainer). For an overview of your administered schools, go to the "schools"-tab in the admin dashboard.
info

\subsection{TUMOnline School vs. GoCast School}

TUMOnline has a strict hierarchical structure for its organizations (one school has multiple departments; one department has multiple chairs; one chair has multiple courses ...).
%
% On a side node, TUMOnline has 7 schools, 29 departments and 487 chairs.
While GoCast is mainly used by the TUM, in principle it doesn't need to differentiate between organizational types that strictly. Organizations are only relevant when it comes to distributing the livestreams and recordings of a certain entity to that entity's resources (e.g., Workers/Runners and VOD Services). Hence, the introduction of GoCast's "schools" which represent an entity responsible for processing data. In practice, this is most of the time a TUMOnline school, however, one can also create a GoCast "school" for a department, chair or smaller organization which is subordinated to another organization, depending on the specific situation.

Here's an example to illustrate this in a more detailled way:
The TUMOnline "School of Management" (SOM) wants to start using GoCast. Hence, the SOM's IT team contacts the admins of GoCast who then create a new GoCast "school" of type TUM School and assign the SOM IT team as maintainers.
The subordinated "Chair of Financial Management and Capital Markets" (FA), however, has its own data center and wants to host its lectures with its own resources. In this case, either one of the SOM maintainers or the RBG can create a new GoCast "school" as a child of the SOM school and accordingly assign new maintainers from the FA-team. Now, the FA-team have full control over their sub-school and can connect their own resources from their data center with GoCast, independently of the SOM.

% The idea is the following: To avoid one entity having to manage and process all streaming data for the entire university (or multiple universities), GoCast is distributed to multiple entities. Each entity (aka GoCast 'school') has so-called maintainers (users with the maintainer user role) that are allowed to manage the school's resources such as Workers/Runners, VoD Services, etc.


% One maintainer can maintain multiple schools.
% The following school-related actions are allowed by a maintainer of a school:

%     Create, update or delete school

%     Create new tokens for that school (required to add new resources)

%     Manage school's resources

%     Manage school's maintainers




\section{Distributed Resources}

Now that the user role system had been updated, the next step was to find a solution to have the different resources such as Workers and Runners be able to be connected by a school to the main cluster of Workers and Runners independently of the others, but at the same time be able to process and distribute requests between each other regardless of the school they are in. This section explains in detail how each subsystem works as part of the distributed GoCast system. 

\subsection{Workers and Runners}
To set up a distributed network of Workers and Runners, the first step was to update the way that Workers and Runners can be connected to the main network of GoCast. To do this, a school maintainer can create a new school-token, a \ac{JWT} that expires after seven hours and allows its owner to connect new resources for the school (see Fig. \ref{fig:school-token}).

\begin{figure}[htpb]
    \centering
    \includegraphics[width=390pt]{images/SchoolToken.png}
    \caption[Example School Token]{Example School Token}\label{fig:school-token}
\end{figure}

When a maintainer starts a new Worker or Runner, he can include this token in the containers environment (e.g., via a \texttt{docker-compose.yml} file or by passing it as an argument directly to Docker using \texttt{-e Token=<...>}). When starting up, the Worker or Runner sends a requests to join the Worker or Runner pool of GoCast using the school-token and additional data (e.g., host name, IP or FQDN address, VM workload, etc.) and - if successful - receives a token without expiration which is then used to identify all subsequent requests from and to the Worker or Runner.  

Maintainers can see and manage the current status of their registered resources in the Resource-Dashboard of GoCast (see Fig. \ref{fig:resource-dashboard}).

\begin{figure}[htpb]
    \centering
    \includegraphics[width=390pt]{images/ResourceDashboard.png}
    \caption[Resources Dashboard]{Resources Dashboard}\label{fig:resource-dashboard}
\end{figure}

Now, whenever a new lecture is being recorded or uploaded, the main GoCast \ac{API} selects an available worker for this school and addresses it given the host name and IP address that the worker has registered when first connecting to GoCast. For more details on how the distribution of tasks works, see Section \ref{section:shared-resources}.

When the Worker or Runner receives such a request, it initiates the video transcoding process using FFmpeg. The transcoding is performed based on the specific stream version (e.g., \texttt{CAM}, \texttt{PRES} or \texttt{COMB}), adjusting parameters like compression level and priority (niceness) accordingly. The process is monitored in real-time, with progress being reported back to the GoCast system, ensuring that any issues are retried with backoff strategies and that the final transcoded video meets the expected duration and quality standards.

\subsection{VOD Service and Edge Server}

Once the Worker or Runner has completed the transcoding process, the VOD Service starts detecting silent sections in the recordings so that the user will be able to skip these and get directly to the start of the actual lecture. Then, it packages them to a \ac{HLS} stream and pushes the packed stream to the school's recording storage.

To ensure easy access to these recordings, each school also needs to have its own Edge Server set up. The VOD Service at each school is responsible for managing the processing, packaging, and storage of the recordings locally, while the Edge Server is responsible for distributing the streams. This also has the advantage that each school maintains control over its data, and reduces latency by storing the recordings closer to the end-users (assuming that a user is more likely to be closer to his school than to others).
Additionally, the Edge Server acts as a local cache for the school's content. When students or faculty access recordings, the Edge Server destributes the content directly from the local storage, reducing the load on the central servers.

The dependency on Workers or Runners for transcoding means that the efficiency and speed of the VOD Service and Edge Server setup depend on the availability and performance of these services. Therefore, it is necessary that a school has enough Workers or Runners to support the number of recorded lectures.
If a school has many concurrent viewers, it might need to consider having multiple Edge Servers running, as they have a limited bandwidth and can only serve approximately . When network traffic to worker nodes exceeds the available bandwidth, the architecture might look like the example shown in Fig.\ref{fig:edge-network}.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\linewidth]{images/EdgeNetwork.png}
    \caption[Edge Server as proxy and cache node]{Edge Server as proxy and cache node}\label{fig:edge-network}
\end{figure}

% \subsection{Ingest Servers}
% TODO: How they are registered and distributed by the RTMP-Proxy
% \subsection{Voice Service}

\subsection{RTMP-Proxy}

GoCast supports not just recordings from the lecture hall or manual uploads, but also so called self-streams. The idea behind self-streams is, that any lecturer can go live at any given time and stream from his own device without having to be in the lecture hall. In past, to do this, a lecturer had to go open GoCast, go to one his courses' page, select a lecture, and copy a link (such as: \texttt{rtmp://worker.example.com:1935/cs-123? secret=5caa9d6447564cb5822995888e224f9a}) together with a secret stream key into a streaming software like OBS Studio. With the introduction of the school system there are several reasons why this can't work:

\begin{enumerate}
    \item The URL might change depending on the availability of workers and schools.
    \item Having to copy and paste a new URL every time a lecturer wants to start a stream can become annoying.
    \item For lecturers who teach at multiple schools (e.g., the Chair of Databases and the Chair of Bioinformatics), managing different URLs for each school would be inconvenient.
\end{enumerate}

The solution for this is the RTMP-Proxy microservice. In a nutshell, the RTMP-Proxy acts as a router-like service that accepts all \ac{RTMP} self-stream requests and redirects them accordingly. With this, all a lecturer has to is creating a new personal Token (see Fig.\ref{fig:personal-token}) and copy the token together with the target URL of the RTPM-Proxy into a streaming software only once.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{images/PersonalToken.png}
    \caption[Creation of Personal Token and RTMP-Proxy URL]{Creation of Personal Token and RTMP-Proxy URL}\label{fig:personal-token}
\end{figure}

After this initial setup, the lecturer can go live whenever they want by simply using the pre-configured URL and token.

The internal process of the RTMP-Proxy is as follows: 

\begin{enumerate}
    \item An incoming \ac{RTMP} request is received: 
    \texttt{rtmp://rtmp-proxy.example.com:1935/ <user-id>?token=<personal-token>}.
    \item The RTMP-Proxy sends a self-stream request to the main GoCast \ac{API} with the provided \texttt{user-id} and \texttt{personal-token}.
    \item The GoCast \ac{API} returns the details of the school from which the user is streaming, including the \ac{RTMP} URL of the ingest worker currently available for that school.
    \item The RTMP-Proxy redirects the \ac{RTMP} request to the retrieved \ac{RTMP} URL.
    \item The school's ingest worker detects whether the lecturer has a scheduled lecture for the current time slot or an upcoming lecture and automatically starts the live-stream for viewers.
\end{enumerate}

While the RTMP-Proxy is far from perfect, it is meant as a prototype of a possible solution to stream via \ac{RTMP} in a distributed network where the final target address of the stream request is unknown when making the request. As the RTMP-Proxy is independent of other services in the GoCast environment (besides the main \ac{API}), it can be scaled very easily by deploying multiple RTMP-Proxies and having for example a Round-robin DNS distribute the load accordingly.

\section{Shared Resources}\label{section:shared-resources}

This section is meant to explain in detail different approaches for the allocation of resources in the GoCast network. The main focus is on the task distribution to Workers and Runners when receiving new video upload requests.

To understand the three approaches presented, it is important to explain in detail how a school actually "owns" a certain resource such as a Worker. As described in Section\ref{section:rbac}, schools are 

3 approaches:

1. Plain

2. Recursive + schools

3. Min/Max problem

TODO: How the distribution of tasks works. (+ operations research)