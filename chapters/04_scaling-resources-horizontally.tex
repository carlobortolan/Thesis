% !TeX root = ../main.tex
\chapter{Scaling Video Streaming Architecture}\label{chapter:videostreaming}

This chapter focuses on how video streaming actually works, what processes are involved in designing and deploying the infrastructure of a video streaming service and what its potentials and challenges with regard to scalability and performance are.   

\section{Video Streaming}\label{section:video-streaming}

The following section explains and compares the most important technical concepts of video streaming.

\subsection{Video Streaming Types}\label{subsection:video-streaming-types}
The concept of video streaming was already present during the early years of television. However, nowadays, video streaming is mostly used to describe the transmission of video and audio data over the internet. In the following, the most important types of video streaming are described with their specific technical requirements and challenges:

\begin{itemize}
    \item \textbf{Live Streaming:} This type involves real-time broadcasting of video content, often used for events, gaming and news. As mentioned in the overview of Twitch's architecture, live streaming requires a robust architecture capable of handling many users accessing the same content simultaneously while maintaining low latency. Technologies such as \ac{RTMP} and \ac{HLS} are commonly used. Ingest servers then process incoming streams, encode them in multiple bitrates, and distribute them through \ac{CDN}s to the viewers.
    \item \textbf{Video on Demand:} \ac{VOD} platforms (such as Netflix) allow users to browse and watch video content at any time. This type of streaming relies heavily on efficient storage systems and \ac{CDN} to manage large content libraries and ensure quick access. \ac{VOD} systems often use progressive streaming techniques, where large video files are segmented and streamed in small chunks as the user watches the video. This allows the user to start watching the video without requiring the entire video to be downloaded upfront, saving time and bandwidth, as only the segments that the user watches are downloaded~\parencite{cloud_streaming_trends}.
    \item \textbf{Peer-to-Peer Streaming:} In \ac{P2P} streaming, viewers share the content they are streaming with others, reducing the load on central servers as the load is distributed to the viewers. \ac{P2P} streaming can be difficult to manage because sophisticated algorithms are needed to optimize peer selection and data distribution and to ensure low latency and high reliability across the network~\parencite{p2p}. This type of streaming is often used for decentralized news broadcasting and content sharing (e.g., BitTorrent Live).
    \item \textbf{Webcasting:} Lastly, there is also webcasting, which - to be precise - needs to be separated from the previous streaming types. While normal live streaming or \ac{VOD} platforms serve a video in a 1:N relationship (1 Video is distributed to many viewers), webcasting serves video in an N:M relationship (all webcast participants are sharing their own videos with each other). As webcasting is mainly used for structured events like corporate events, webinars and online meetings (e.g., via Zoom, BBB, or Teams), maintaining a low latency of typically 150ms or less is very important. Higher latency values will result in noticeable delays between video and audio and make real-time communication in online conferences and meetings difficult (in comparison with normal live streaming, where a latency of up to 30 seconds will not be noticed by the viewers).  
    It often involves additional features such as viewer authentication, interaction capabilities and detailed analytics~\parencite{cloud_streaming_trends}.
\end{itemize}

\subsection{Video Transcoding}
Once a video has been received, it needs to be transcoded into different formats to ensure compatibility across platforms while maintaining as much of its original quality as possible. There are different types of transcoding; the most important are:

\begin{itemize}
    \item \textbf{Standard Transcoding} involves changing the video compression standard of the video, e.g., by switching between different codecs such as H.265/HEVC to support different devices. It is important to note that when changing codec, it has to decode the bitstream with the old codec first and then encode it again with the new codec, making the entire operation very compute-intensive.~\parencite{codec_transcoding}.
    \item \textbf{Bitrate Transcoding/Transrating} adjusts the bitrate of a video stream to balance the quality of the video and subsequent bandwidth usage. For instance, a high-resolution video may need to be transcoded to a lower bitrate for users with limited bandwidth. \ac{ABR} is often used for this, as multiple bitrate streams are generated, leaving the decision of which frame rate to select up to the client based on current network conditions~\parencite{transcoding}.
    \item \textbf{Spatial Transcoding/Transsizing} modifies a video's resolution to match the viewing device's capabilities. For example, a 4K video may be downscaled to 1080p or 720p for playback on devices that do not support higher resolutions~\parencite{cloud_streaming}.
    \item \textbf{Frame Rate Transcoding} modifies the number of frames per second in a video file - usually by reducing the number of frames by removing frames in a certain interval so that the viewer does not notice a difference~\parencite{transcoding}.
\end{itemize}

Also, it needs to be mentioned that there are different compression classes used as part of the transcoding process, with the two primary classes being:

\begin{itemize}
    \item \textbf{Lossy Compression:} Compression algorithms of this class reduce a file's size by removing parts of the video data. 
    While this can affect the video quality, it decreases the overall bandwidth usage, as the video files are now smaller, making them easier to cache and requiring less data to be streamed to the end user. Popular lossy codecs include \href{https://www.itu.int/rec/T-REC-H.264}{H.264}\footnote{\url{https://www.itu.int/rec/T-REC-H.264}}, \href{https://www.itu.int/rec/T-REC-H.265}{H.265}\footnote{\url{https://www.itu.int/rec/T-REC-H.265}} and \href{https://developers.google.com/media/vp9}{VP9}\footnote{Developed by Google and published 2013 to compete with H.265 and used mainly by YouTube. See also: \url{https://developers.google.com/media/vp9} and \url{https://www.webmproject.org/vp9}}~\parencite{combression}.
    \item \textbf{Lossless Compression:} This compression class reduces file size without losing data, preserving the original quality. However, the compression ratios are typically much lower compared to lossy methods. Codecs like \href{https://www.apple.com/final-cut-pro/docs/Apple_ProRes.pdf}{Apple ProRes}\footnote{\url{https://www.apple.com/final-cut-pro/docs/Apple_ProRes.pdf}} and \href{https://github.com/FFmpeg/FFV1/blob/master/ffv1.md}{FFV1}\footnote{\url{https://github.com/FFmpeg/FFV1/blob/master/ffv1.md}} are examples of lossless compression used in professional environments such as film productions or studio-music recordings~\parencite{combression}.
\end{itemize}

While Transcoding changes the file's contents, converting the video file format (e.g., from .avi to .mp4) is often referred to as Format Transcoding/Transmuxing. However, strictly speaking, this is not part of Transcoding as this operation changes the container and not the actual content, although it often includes re-encoding audio streams to formats like AAC or Opus, depending on the target platform~\parencite{transcoding}.

\subsection{Delivery Networks}
Delivery networks ensure that video content reaches end-users quickly and reliably. The two main approaches for this are \ac{CDN}s and \ac{P2P} networks:

\ac{CDN}s are a network of geographically distributed servers that cache video content close to end-users. When a request is made (e.g., a user wanting to stream a video), it is routed to the closest \ac{CDN} node, which delivers the content with minimal latency. \ac{CDN}s like Amazon CloudFront, Azure CDN and Google Cloud CDN are designed to handle massive amounts of traffic and can scale dynamically based on demand~\parencite{cdn_basic}.
Technically, \ac{CDN} works by replicating video content across multiple nodes in different locations. Each node (or edge server) holds a copy of the content, reducing the distance data must travel to the end-user, improving load times and reducing latency. Advanced \ac{CDN}s also use techniques such as Anycast routing, where a single IP address is shared by multiple servers and requests are automatically routed to the nearest or least-loaded server~\parencite{cdn_basic}.
    
As mentioned before, \ac{P2P} streaming uses viewers' bandwidth to deliver content, reducing the load on central servers~\parencite{cdn_basic}, as instead of relying on one central server, each user in the network shares parts of the video stream with others.
However, \ac{P2P} networks require complex algorithms to ensure optimal peer selection and data distribution. Algorithms like BitTorrent's Tit-for-Tat, which motivate users to share data by rewarding them with faster downloads\footnote{\ac{P2P} clients may prefer sending data to peers that send data back to them, "rewarding" them with faster download speeds which reduce loading times and allow the viewer to increase the video resolution.}, are often used for this. Additionally, to keep the latency as low as possible, \ac{P2P} networks use buffer management strategies and protocols designed to minimize the delay in data exchange between peers~\parencite{p2p}.

\subsection{Security}

Most video streaming services work with licensed data such as movies or copyrighted music and must guarantee that their content is protected from unauthorized access, piracy and other malicious activities.
There are three factors that need to be considered when evaluating the security of a streaming service: 

First, ensure that only authorized users can access the video content. This is often achieved through \ac{JWT} based authentication, which generates a unique \ac{JWT} for each session or using access delegation protocols such as OAuth. What's important is that the authentication and access control is strictly applied on all levels - not just when a user accesses the streaming service, but also on an individual request basis to avoid an authorized user accessing forbidden scopes. A relevant example of this is streaming services that require authentication for a user to access the platform but then do not require any authentication for accessing specific content. A malicious user could exploit this to access hidden content or content for which he is not authorized. Also, using the example of a \ac{VOD} platform, this could be used to download and share original high-resolution videos by checking the source URL of a video and accessing the full video file from there.
Alternatively, there are also other attack vectors such as possible file and directory discovery where "\textit{adversaries may enumerate files and directories or may search in specific locations of a host or network share for certain information within a file system}"~\parencite{mitre}. A simple solution to defend from this kind of access would be to ensure that any content on the site is only accessible with a valid \ac{JWT} (or any other form of token) that is limited with regard to the scope and validity.

Secondly, to prevent piracy or any unwanted activities, one can use \ac{DRM} tools like \href{https://www.microsoft.com/playready/documents}{Microsoft's PlayReady}\footnote{\url{https://www.microsoft.com/playready/documents}}, \href{https://developer.apple.com/streaming/fps}{Apple's FairPlay}\footnote{\url{https://developer.apple.com/streaming/fps}} and \href{https://developers.google.com/widevine}{Google's Widevine}\footnote{\url{https://developers.google.com/widevine}} protect content by controlling how it is accessed and used. \ac{DRM} typically involves encrypting the content and then using licenses to grant authorized users the ability to decrypt and play the content. The encryption process usually involves \ac{AES} with 128-bit or 256-bit keys. When a user attempts to play \ac{DRM}-protected content, the video player requests a license from a \ac{DRM} server. If the user is authorized, the server provides the license, which includes the decryption key. The player then decrypts the content and plays it~\parencite{drm}. However, as many of these \ac{DRM} services cost up to several cents per \ac{DRM} license or several thousand dollars per year, one needs to consider if it is worth the additional effort and expense.

Lastly, protecting the infrastructure itself is critical. Beyond DRM, video streams are often encrypted during transmission (e.g., using \ac{TLS}) to protect against interception. Additionally, standard security practices such as IP whitelisting, port restriction, the use of firewalls and regular security audits can help mitigate potential threats.

\section{Cloud-Based Video Streaming}

In 2008 Netflix started to migrate all its services to the cloud (\ac{AWS}) to move away from vertically\footnote{\textit{vertically} refers to scaling up by increasing the current resources' power (e.g. upgrading GPUs or Storage)} scaled single points of failure, like relational databases, towards highly reliable, horizontally\footnote{\textit{horizontally} refers to scaling out by adding additional nodes or machines to the current infrastructure} scalable, distributed systems in the cloud~\parencite{netflix_aws}. Since then, deploying and scaling using third-party cloud services has become the standard. This section goes into detail on how this can be especially useful for a video streaming infrastructure. 

\subsection{Architecture}
Cloud-based video streaming typically involves multiple layers, each responsible for different aspects of the streaming process:

\begin{itemize}
    \item \textbf{Content Storage:} Platforms like \ac{AWS} S3, Google Cloud Storage and Azure Blob Storage offer scalable and reliable storage solutions for video content. These platforms use object storage systems designed to handle massive amounts of unstructured data. Data is often stored in multiple copies across different geographic locations to ensure durability and availability~\parencite{cloud_streaming}. (see also \autoref{tab:comparison_storage_delivery})
    
    \item \textbf{Encoding and Transcoding:} Cloud services like \ac{AWS} Elemental MediaConvert, Google Cloud Video Intelligence and Azure Media Services handle the encoding and transcoding of video into various formats and bitrates. These services are designed to scale automatically based on the volume of processed content, ensuring that even large video collections can be encoded efficiently~\parencite{cloud_streaming}.
    
    \item \textbf{Content Delivery Networks (CDNs):} As mentioned earlier, cloud-based \ac{CDN}s, such as \ac{AWS} CloudFront, Google Cloud CDN and Azure CDN are often used to deliver video content globally.~\parencite{cloud_streaming}.
    
    \item \textbf{\ac{API}s and Microservices:} Cloud-based architectures often rely on microservices, with each service serving a specific purpose, such as user authentication, recommendation engines and analytics. These microservices communicate through \ac{API}s, which can be easily deployed, scaled and updated independently using systems cloud engines, such as Azure Kubernetes Service or Google Kubernetes Engine~\parencite{cloud_streaming}.
\end{itemize}

\subsection{Comparison: In-house Storage vs. Cloud Storage}

When designing the architecture of a video streaming service, an important decision is to decide how and where the actual video data should be stored. The following table (\autoref{tab:comparison_storage_delivery}) and subsection summarize the key characteristics of these three approaches in the context of video streaming. 

\textbf{In-House Storage} involves managing physical storage infrastructure. This allows the owner full control over how the data is managed and allows also for highly customizable configurations, which can be a hard requirement for organizations bound to strict compliance regulations. However, it demands significant upfront capital expenditure and ongoing maintenance. Also, its scalability is limited by the physical infrastructure, requiring careful planning and potentially costly upgrades making short-term adjustments to handle flexible loads very difficult. 

\textbf{Cloud Storage}, on the other hand, offers a flexible and on-demand scalable solution managed by third-party providers like AWS, Google Cloud, or Microsoft Azure. It allows video streaming providers to scale their storage needs dynamically without significant upfront investment or configuration effort, as most use cases are already covered by default configurations and extensive documentation and support if necessary. However, while often operating on a pay-as-you-go model, it comes at the cost of limited control over data and potential security concerns that are dependent on the cloud provider's policies. Cloud storage also introduces variability in latency depending on the location of data centers relative to users. While it is cost-effective in the short term, long-term costs and migration issues due to vendor lock-in can accumulate as data usage grows \parencite{cloud_streaming_cost}.

While not exactly being an alternative storage method, \textbf{\ac{P2P}} systems are highly scalable and can be a very cost-effective way to distribute and store data. However, they come with challenges related to security, data integrity and performance consistency, as they rely on the cooperation of peers~\parencite{p2p}. Latency and quality of service can vary significantly depending on network conditions and peer availability, making \ac{P2P} suitable for environments where cost savings are prioritized and some variability in performance is acceptable.

\begin{table}[h]
    \centering
    \caption{Comparison of In-House Storage, Cloud Storage and \ac{P2P} for Video Streaming}
    \label{tab:comparison_storage_delivery}
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{|l|c|c|c|}
            \hline
            \textbf{Factor} & \textbf{In-House Storage} & \textbf{Cloud Storage} & \textbf{P2P} \\ \hline
            \textbf{Control} & Full & Limited & Distributed \\ \hline
            \textbf{Security} & High, customizable & Provider-dependent & Lower, peer-based \\ \hline
            \textbf{Scalability} & Hardware-dependent & Virtually unlimited & Highly scalable \\ \hline
            \textbf{Costs} & High upfront, low ongoing & Pay-as-you-go & Low infrastructure cost \\ \hline
            \textbf{Performance} & Low latency & Variable & Variable \\ \hline
            \textbf{Management} & High maintenance & Low maintenance & Complex, peer-managed \\ \hline
            \textbf{Redundancy} & Customizable, local & High, managed by provider & Peer-based, variable \\ \hline
            \textbf{Compliance} & Easier to customize & Provider-dependent & Difficult to enforce \\ \hline
            \textbf{Latency} & Low & Moderate to high & Variable \\ \hline
            \textbf{Use Case} & Enterprise & Scalable, flexible & Cost-sensitive, user-driven \\ \hline
        \end{tabular}
    }
\end{table}

\section{Database Sharding}\label{section:db-sharding}
While the previous sections described general concepts of video streaming, this section is meant to focus on a more specific problem that services face when scaling up. 
With an increasing number of uploaded videos, there is the issue of running out of storage space (see also \autoref{subsection:yt-scalability-challenges}). This can easily be solved by buying either a larger storage system or extending the current storage system with a cluster of storage systems. However, the options are more limited when running out of space in a relational database. One can still upscale the current database, but for large amounts of data - especially for services such as YouTube or Netflix - a single database is not enough. Adding additional databases can be a complex task, especially when trying to maintain \ac{ACID} properties. 
A possible solution to this is database sharding: a technique where large databases are distributed across multiple servers. In the context of video streaming, sharding can help store large amounts of content metadata, data generated by user interactions, and playback logs more efficiently.

\subsection{Overview of Database Sharding}
Database sharding, also known as horizontal partitioning, involves dividing a large database into smaller databases, so-called shards. Now the data can be processed and accessed in parallel, as each shard is stored on a separate server. This technique is especially useful for scaling databases that handle massive amounts of data, as is common in video streaming platforms. Database Sharding can be implemented in a shared-nothing architecture, where each shard operates independently, thus avoiding the contention issues typically associated with shared-disk clustered databases~\parencite{db_sharding}. This independence ensures that the failure of one shard does not affect the others, improving fault tolerance and availability.

A sharded database architecture divides data among multiple data nodes based on a partitioning scheme. Some of the most common partitioning strategies include range-based sharding, where data is divided based on the value range of a key and hash-based sharding, where a hash function is applied to a key to determine the shard placement~\parencite{db_sharding}. This allows the distribution of the data in a more balanced way and can significantly improve read and write operations, as each server now only needs to manage a reduced amount of data.

\subsection{Advantages of Sharding}
One of the main advantages is scalability, as with increasing data, new shards can be added without requiring significant changes to the application or database architecture. This would allow video streaming platforms to handle increasing loads efficiently, whether it be due to more users, more content, or both~\parencite{db_sharding}.

Another advantage of sharding is fault tolerance. If one shard becomes unavailable or has a data loss due to any kind of hardware or network failures, the replicated data on another available node can take over, ensuring uptime of service, as the data is typically replicated across multiple nodes~\parencite{db_sharding}.

\noindent Sharding also improves manageability and maintainability by dividing the database into smaller, more manageable units. Database admins can perform maintenance tasks, such as migrations, backups or schema updates on individual shards without affecting the entire system. This modular approach reduces the risk of large-scale disruptions~\parencite{db_sharding}. Of course, there remains the risk that a misconfiguration in one database can lead to errors across the systems, and unnoticed errors may spread from one database to others, complicating the debugging and repair of the databases.

\subsection{Challenges and Considerations}
The most critical challenge is maintaining consistency across shards. In a distributed environment, ensuring that all shards reflect the most recent data state can become difficult, especially in case of network partitions or node failures. To try to counter this, techniques such as distributed transactions and eventual consistency models are often used, but they will again add additional complexity to the system~\parencite{skyline_joins}.

Another challenge is re-balancing shards as the data grows or usage patterns change. Over time, some shards may become "hot" (i.e., they handle a disproportionate amount of traffic compared to other shards), potentially leading to new performance bottlenecks. Re-balancing would require redistributing data across shards to ensure even load distribution~\parencite{db_sharding_cloud}.

Lastly, sharding complicates query processing leading to potentially slower execution of cross-shard queries, where data from multiple shards needs to be processed. While there are special approaches to optimize distributed queries such as \textit{Reference/Distributed Table Joins} or \textit{Remote Distributed Table Joins}, careful query planning and optimization are still required to minimize the impact~\parencite{db_sharding_joins}.

To conclude this section, it is important to repeat that database sharding is a great option to scale database systems while improving fault tolerance and manageability. But at the same time, it also introduces challenges such as consistency management, shard re-balancing, and complex query processing. In the end, most companies or services with database scalability problems will more likely increase their current database compute and storage power - and worry about other approaches, such as sharding only when they reach a limit with their current system~\parencite{db_sharding_newsql}.  