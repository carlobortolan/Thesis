% !TeX root = ../main.tex
\chapter{Scaling Video Streaming Architecture}\label{chapter:videostreaming}

This chapter focuses on how video streaming actually works, what processes are involved in designing and deploying the infrastructure of a video streaming service and what its potentials and challenges in regards to scalability and performance are.   

\section{Video Streaming}

In the following section, the most important technical concepts of video streaming are explained and compared.

\subsection{Video Streaming Types}
The concept of video streaming was already present during the early years of television. However, nowadays video streaming is mostly used to describe transmitting videos over the Internet. In the following are described the most important types of video streaming with their specific technical requirements and challenges:

\begin{itemize}
    \item \textbf{Live Streaming:} This type involves real-time broadcasting of video content, often used for events, gaming and news. As already mentioned in the overview of Twitch's architecture, live streaming requires a robust architecture capable of handling high concurrency and maintaining low latency. Technologies such as \ac{RTMP} and \ac{HLS} are commonly used. Ingest servers then process incoming streams from broadcasters, encode them in multiple bitrates, and distribute them through \ac{CDN}s to ensure smooth delivery to viewers.
    \item \textbf{Video on Demand:} \ac{VOD} services (such as YouTube, Netflix, etc.) allow users to select and watch video content at any time. This type of streaming relies heavily on efficient storage systems and \ac{CDN} to manage large content libraries and ensure quick access. \ac{VOD} systems often use progressive streaming techniques, where large video files are segmented and is streamed in small chunks as the user watches the video, allowing for a more efficient viewing experience without requiring the entire video to be downloaded upfront~\parencite{cloud_streaming_trends}.
    \item \textbf{Webcasting:} Webcasting is similar to live streaming but typically used for structured events like corporate meetings, webinars, and online training (e.g., via Zoom, BBB or Teams). It often involves additional features such as viewer authentication, interaction capabilities, and detailed analytics. The architecture must support both high scalability and customization for enterprise-level use~\parencite{cloud_streaming_trends}.
    \item \textbf{On-Demand Streaming:} A subset of \ac{VOD}, where content is streamed as it is requested by users without pre-buffering. This type of streaming requires efficient backend systems to handle large numbers of concurrent requests while minimizing latency~\parencite{cloud_streaming}.
    % \item \textbf{Progressive Download:} In this method, video content is downloaded in segments and begins playback as soon as enough data has been buffered. This approach is useful in scenarios with fluctuating network conditions, as it allows the video to play without interruption~\parencite{progressive_download}.
    \item \textbf{Peer-to-Peer Streaming:} In \ac{P2P} streaming, viewers share the content they are streaming with others, reducing the load on central servers and enhancing bandwidth efficiency. \ac{P2P} streaming can be challenging to manage due to the need for sophisticated algorithms to optimize peer selection, data distribution, and to ensure low latency and high reliability across the network~\parencite{p2p}. This type of streaming is often used by sports or news broadcasters.
\end{itemize}

\subsection{Video Transcoding}
Once a video has been recorded, it needs to be transcoded into different formats to ensure compatibility across various devices and platforms while maintaining as much of its original quality as possible. The process can be broken down into five operations:

\begin{itemize}
    \item \textbf{Standard Transcoding} involves changing video compression standard of the video, e.g., by switching between different codecs such as H.265/HEVC or H.262/MPEG-2 to support different devices. It is important to note that when changing codec, it has to decode the bitstream with the old codec first and then encode it again with the new codec, making the entire operation very compute-intensive.~\parencite{codec_transcoding}.
    \item \textbf{Bitrate Transcoding/Transrating} adjusts the bitrate of a video stream to balance the quality of the video and subsequent bandwidth usage. For instance, a high-resolution video may need to be transcoded to a lower bitrate for users with limited bandwidth. \ac{ABR} is often used for this, as multiple bitrate streams are generated, leaving the decision of which frame rate to select up to the client based on current network conditions~\parencite{transcoding}.
    \item \textbf{Spatial Transcoding/Transsizing} modifies the resolution of a video to match the capabilities of the viewing device. For example, a 4K video may be downscaled to 1080p or 720p for playback on devices that do not support higher resolutions~\parencite{cloud_streaming}.
    \item \textbf{Frame Rate Transcoding} modifies the number of frames in a video file - usually by reducing the number of frames by removing frames in a certain distance so that the human visual system doesn't notice a difference~\parencite{transcoding}.
    \item While Transcoding changes the contents of the file, converting the video file format (e.g., from .avi to .mp4), is often referred to as \textbf{Format Transcoding/Transmuxing} as it changes the container and not the actual content. This step often includes re-encoding audio streams to formats like AAC or Opus, depending on the target platform~\parencite{transcoding}.
\end{itemize}

Also, it needs to be mentioned that there are different compression techniques used as part of the transcoding process, with the two primary methods being:

\begin{itemize}
    \item \textbf{Lossy Compression:} This technique reduces file size by removing some of the video data, which can affect quality but significantly decreases bandwidth usage. Popular lossy codecs include H.264, H.265, and VP9~\parencite{combression}.
    \item \textbf{Lossless Compression:} On the other hand, lossless compression reduces file size without any loss of data, preserving the original quality. However, the compression ratios are typically much lower compared to lossy methods. Codecs like Apple ProRes and FFV1 are examples of lossless compression used in professional environments~\parencite{combression}.
\end{itemize}

\subsection{Delivery Networks}
Delivery networks ensure that video content reaches end-users quickly and reliably. The two main approaches for this are \ac{CDN}s and \ac{P2P} networks:

\ac{CDN}s are a network of geographically distributed servers that cache video content close to end-users. When a user requests content, the request is routed to the nearest \ac{CDN} node, which delivers the content with minimal latency. \ac{CDN}s like Amazon CloudFront, Azure CDN, and Google Cloud CDN are designed to handle massive amounts of traffic and can scale dynamically based on demand~\parencite{cdn_basic}.
Technically, \ac{CDN}s work by replicating video content across multiple nodes in different locations. Each node (or edge server) holds a copy of the content, reducing the distance data must travel and therefore improving load times and reducing latency. Advanced \ac{CDN}s also use techniques such as Anycast routing, where a single IP address is shared by multiple servers, and requests are automatically routed to the nearest or least-loaded server~\parencite{cdn_basic}.
    
As mentioned before, \ac{P2P} streaming takes advantage of the bandwidth of viewers to distribute content. Instead of relying on a central server, each user in the network shares parts of the video stream with others. This method reduces the load on central servers and can improve scalability, especially for live streaming with high concurrency~\parencite{cdn_basic}.
However, \ac{P2P} networks require complex algorithms to ensure optimal peer selection and data distribution. Algorithms like BitTorrent's Tit-for-Tat, which motivate users to share data by rewarding them with faster downloads, are often used for this. Additionally, latency in \ac{P2P} networks is managed through buffer management strategies and protocols designed to minimize the delay in data exchange between peers~\parencite{p2p}.

\subsection{Security}

Most video streaming services handle licensed data such as movies or copyrighted music and need to guarantee that its content is protected from unauthorized access, piracy, and other malicious activities.
There are three factors that need to be considered when evaluating the security of a streaming service: 

First, ensuring that only authorized users can access certain content. This is most often achieved through various methods, including \ac{JWT} based authentication, which generates a unique \ac{JWT} for each session, or protocols such as OAuth. What's important is, that the authentication and access control is strictly applied on all levels - not just when a user accesses the streaming service, but also on an individual request basis to avoid that an authorized user can access forbidden scopes. A relevant example for this is streaming services that require authentication for a user to access the platform, but then don't require any authentication for accessing specific content. A malicious user could use this to, for example, download full videos by checking the source URL of the video and simply accessing the full video from there. Alternatively, there are also other attack vectors such as possible file and directory discovery where "\textit{adversaries may enumerate files and directories or may search in specific locations of a host or network share for certain information within a file system}"~\parencite{mitre}. A simple solution to defend from this kind of access would be to ensure that all content can only be accessed with a valid \ac{JWT} (or any other form of token) that is limited with regards to the scope and validity.

Secondly, to prevent piracy, or any unwanted activities, one can use \ac{DRM} tools like Microsoft's PlayReady, Apple's FairPlay, and Google's Widevine protect content by controlling how it is accessed and used. \ac{DRM} typically involves encrypting the content and then using licenses to grant authorized users the ability to decrypt and play the content. The encryption process usually involves \ac{AES} with 128-bit or 256-bit keys. When a user attempts to play \ac{DRM}-protected content, the video player requests a license from a \ac{DRM} server. If the user is authorized, the server provides the license, which includes the decryption key. The player then decrypts the content and plays it~\parencite{drm}. However, as many of these \ac{DRM} services require a paid license, one needs to consider if it is worth the additional effort and expense.

Lastly, protecting the infrastructure itself is critical. Beyond DRM, video streams are often encrypted during transmission (e.g., using \ac{TLS})to protect against interception. Techniques such as IP whitelisting, port restriction, and the use of firewalls are standard practices. Additionally, regular security audits and real-time monitoring can help identify and mitigate potential threats.

% \section{Deployment}
% Efficient deployment strategies are essential for scaling video streaming services to meet user demands while maintaining performance and reliability. Depending on the size and extent of the project, one approach might be better than another and help save significant time when developing and deploying the different microservices of a streaming service (more on that in \autoref{chapter:scaling_tumlive}). This section is meant to give an overview of the most popular deployment technologies and discuss their functionality, use cases and monitoring tools.

% \subsection{Docker}
% TODO
% Docker is a containerization platform that allows developers to package applications and their dependencies into containers. These containers can then be deployed consistently across various environments, ensuring that the application behaves the same way regardless of where it runs.

% In video streaming, Docker is used to encapsulate encoding services, web servers, databases, and other components. Containers can be easily scaled across multiple servers, enabling the deployment of video streaming applications in a distributed and scalable manner. Docker images can be versioned and deployed through Continuous Integration/Continuous Deployment (CI/CD) pipelines, allowing for rapid updates and rollbacks without downtime~\parencite{docker_deployment}.

% \subsection{Docker Swarm}
% TODO
% Docker Swarm extends Docker's capabilities by enabling clustering and orchestration of containers across multiple hosts. It allows for the seamless management of a large number of containers, automatically distributing workloads across a cluster to optimize resource utilization.

% In the context of video streaming, Docker Swarm can be used to manage the various microservices involved in streaming, such as encoding, storage, and delivery. Swarm handles tasks like load balancing, scaling containers up or down based on demand, and ensuring high availability by replicating services across different nodes~\parencite{docker_swarm_deployment}.

% \subsection{Kubernetes}
% TODO
% Kubernetes is an advanced orchestration platform that automates the deployment, scaling, and management of containerized applications. Kubernetes offers more advanced features than Docker Swarm, including automatic scaling, self-healing, and rolling updates.

% For video streaming services, Kubernetes can manage thousands of containers across a distributed infrastructure. It ensures that services remain available even if some nodes fail, by automatically rescheduling containers on healthy nodes. Kubernetes' Horizontal Pod Autoscaler can dynamically adjust the number of running containers based on CPU utilization or custom metrics, ensuring that the streaming service can handle fluctuating demand~\parencite{kubernetes_video_deployment}.

% Kubernetes also supports StatefulSets, which are essential for managing stateful applications like databases, ensuring that each instance maintains its identity across reschedules. This is particularly important for maintaining the consistency and availability of data in video streaming platforms~\parencite{statefulset_kubernetes}.

% \subsection{Monitoring Tools}
% To keep an overview of what the current status of the system is and analyze past errors and logs, monitoring is essential. Several (open source) tools are commonly used in the industry:

% \begin{itemize}
%     \item \textbf{Network Performance} tools like \href{https://github.com/NagiosEnterprises/nagioscore}{Nagios} and \href{https://github.com/zabbix/zabbix}{Zabbix} monitor and analyze network traffic, server health and bandwidth to identify bottlenecks and optimize network performance.
    
%     \item \textbf{CDN Monitoring} tools such as \href{https://github.com/DataDog/datadog-agent}{Datadog} track \ac{CDN} performance, latency, and availability, ensuring that content delivery is optimized across different geographic regions.
    
%     \item \textbf{Video Quality Monitoring} tools like \href{https://github.com/ccrisan/streameye}{StreamEye} or \href{https://github.com/Netflix/vmaf}{VMAF (Video Multimethod Assessment Fusion)} assess video quality from the viewer's perspective. These tools analyze factors like resolution, bitrate, and compression artifacts to ensure that the delivered video meets quality standards.
    
%     \item \textbf{Error Tracking} systems like \href{https://github.com/getsentry/sentry}{Sentry}, \href{https://github.com/rollbar?q=&type=all&language=&sort=stargazers}{Rollbar}, and \href{https://github.com/grafana/grafana}{Grafana} detect and report errors in real-time, allowing for rapid response to issues that could affect the streaming experience.
    
%     \item \textbf{Logging and Dashboards} tools like \href{https://github.com/grafana/loki}{Loki}, \href{https://github.com/prometheus/prometheus}{Prometheus}, and \href{https://github.com/grafana/grafana}{Grafana} provide detailed insights into system performance, user behavior, and potential issues. These tools can be integrated into real-time dashboards and connected to third party notification services, offering a clear overview of the entire streaming infrastructure and instant alerts to allow for immediate countermeasures.
% \end{itemize}

\section{Cloud-Based Video Streaming}

In 2008 Netflix started to migrate all its services to the cloud (\ac{AWS}) to move away from vertically scaled single points of failure, like relational databases, towards highly reliable, horizontally scalable, distributed systems in the cloud~\parencite{netflix_aws}. Since then it has become the standard to deploy and scale using third party cloud services. This section goes into detail how this can be especially useful for video streaming infrastructure. 

\subsection{Architecture}
Cloud-based video streaming architecture typically involves multiple layers, each responsible for different aspects of the streaming process:

\begin{itemize}
    \item \textbf{Content Storage:} Platforms like Amazon S3, Azure Blob Storage, and Google Cloud Storage offer scalable and reliable storage solutions for video content. These platforms use object storage systems designed to handle massive amounts of unstructured data. Data is often stored in multiple copies across different geographic locations to ensure durability and availability~\parencite{cloud_streaming}. (see also \autoref{tab:comparison_storage_delivery})
    
    \item \textbf{Encoding and Transcoding:} Cloud services like AWS Elemental MediaConvert, Azure Media Services, and Google Cloud Video Intelligence handle the encoding and transcoding of video into various formats and bitrates. These services are designed to scale automatically based on the volume of content being processed, ensuring that even large video libraries can be encoded efficiently~\parencite{cloud_streaming}.
    
    \item \textbf{Content Delivery Networks (CDNs):} As mentioned earlier, cloud-based \ac{CDN}s such as Amazon CloudFront, Azure CDN, and Google Cloud CDN are essential for delivering video content globally.~\parencite{cloud_streaming}.
    
    \item \textbf{\ac{API}s and Microservices:} Cloud-based architectures often rely on microservices, each responsible for a specific function, such as user authentication, recommendation engines, and analytics. These microservices communicate through \ac{API}s, which can be easily deployed, scaled and updated independently using systems cloud engines such as Azure Kubernetes Service or Google Kubernetes Engine~\parencite{cloud_streaming}.
\end{itemize}

\subsection{Comparison: In-house Storage vs. Cloud Storage}

When designing the architecture of a video streaming service, an important decision is to decide how and where the actual video data should be stored. The following table (\autoref{tab:comparison_storage_delivery}) and subsection summarize the key characteristics of these three approaches in the context of video streaming. 

\textbf{In-House Storage} involves managing own physical storage infrastructure. This allows the owner full control over how the data is managed and allows also for highly customizable configurations, which can be a hard requirement for organizations bound to strict compliance regulations. However, it demands significant upfront capital expenditure and ongoing maintenance. Also, its scalability is limited by the physical infrastructure, requiring careful capacity planning and potentially costly upgrades making short-term adjustments to handle flexible loads very difficult. 

\textbf{Cloud Storage}, on the other hand, offers a flexible and scalable solution managed by third-party providers like AWS, Google Cloud, or Microsoft Azure. It allows video streaming providers to scale their storage needs dynamically without significant upfront investment or configuration effort, as most use cases are already covered by default configurations and extensive documentation and support if necessary. However, while often operating on a pay-as-you-go model, it comes at the cost of limited control over data and potential security concerns that are dependent on the cloud provider's policies. Cloud storage also introduces variability in latency depending on the location of data centers relative to users, and while it is cost-effective in the short term, long-term costs and migration issues due to vendor lock-in can accumulate as data usage grows \parencite{cloud_streaming_cost}.

While not exactly being an alternative storage method, \textbf{\ac{P2P}} systems are highly scalable and can be a very cost-effective way to distribute and store data. However, they come with challenges related to security, data integrity, and performance consistency, as they rely on the cooperation of peers~\parencite{p2p}. Latency and quality of service can vary significantly depending on network conditions and peer availability, making \ac{P2P} suitable for environments where cost savings are prioritized, and some variability in performance is acceptable.

\begin{table}[h]
    \centering
    \caption{Comparison of In-House Storage, Cloud Storage, and P2P for Video Streaming}
    \label{tab:comparison_storage_delivery}
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{|l|c|c|c|}
            \hline
            \textbf{Factor} & \textbf{In-House Storage} & \textbf{Cloud Storage} & \textbf{P2P} \\ \hline
            \textbf{Control} & Full & Limited & Distributed \\ \hline
            \textbf{Security} & High, customizable & Provider-dependent & Lower, peer-based \\ \hline
            \textbf{Scalability} & Hardware-dependent & Virtually unlimited & Highly scalable \\ \hline
            \textbf{Costs} & High upfront, low ongoing & Pay-as-you-go & Low infrastructure cost \\ \hline
            \textbf{Performance} & Low latency & Variable & Variable \\ \hline
            \textbf{Management} & High maintenance & Low maintenance & Complex, peer-managed \\ \hline
            \textbf{Redundancy} & Customizable, local & High, managed by provider & Peer-based, variable \\ \hline
            \textbf{Compliance} & Easier to customize & Provider-dependent & Difficult to enforce \\ \hline
            \textbf{Latency} & Low & Moderate to high & Variable \\ \hline
            \textbf{Use Case} & Enterprise & Scalable, flexible & Cost-sensitive, user-driven \\ \hline
        \end{tabular}
    }
\end{table}

\section{Database Sharding}
While the previous sections described general concepts of streaming, this section is meant to focus on a more specific problem that video streaming services face when scaling up. 
With an increasing number of uploaded videos, there is the issue of running out of storage space. This can easily be solved be buying either a larger storage system or by extending the current storage system with cluster of storage systems. However, when running out of space in a relational database, the options are more limited. One can still upscale the current database, but for large amounts of data - especially for services such as YouTube or Netflix - a single database is not enough. Adding additional databases can however be a complex task, especially when trying to maintain \ac{ACID} properties. 
Hence, a possible solution to this is database sharding: a technique used to distribute large databases across multiple servers. In the context of video streaming, sharding is crucial for handling the vast amounts of data generated by user interactions, content metadata, and playback logs.

\subsection{Overview of Database Sharding}
Database sharding, also known as horizontal partitioning, involves splitting a large database into smaller, more manageable databases, so called shards. Each shard is stored on a separate server, allowing for distributed processing and parallel access to the data. This technique is especially useful for scaling databases that handle massive amounts of data, as is common in video streaming platforms. Database Sharding can be implemented in a shared-nothing architecture, where each shard operates independently, thus avoiding the contention issues typically associated with shared-disk clustered databases~\parencite{db_sharding}. This independence ensures that the failure of one shard does not affect the others, improving fault tolerance and availability.

In a sharded database architecture, data is divided among multiple data nodes based on a partitioning scheme. Common partitioning strategies include range-based sharding, where data is divided based on the value range of a key, and hash-based sharding, where a hash function is applied to a key to determine the shard placement~\parencite{db_sharding}. These strategies allow for balanced data distribution and can significantly improve the performance of read and write operations by reducing the amount of data each server needs to manage.

\subsection{Advantages of Sharding}
One of the main advantages is scalability, as with increasing data, new shards can be added without requiring significant changes to the application or database architecture. This would allow video streaming platforms to handle increasing loads efficiently, whether it be due to more users, more content, or both~\parencite{db_sharding}.

Another advantage of sharding is fault tolerance. If one shard becomes unavailable or has a data loss due to hardware or network failures, the replicated data on another healthy node can take over, ensuring continuous availability of the service, as the data is typically replicated across multiple nodes~\parencite{db_sharding}.

Sharding also improves manageability and maintainability by dividing the database into smaller, more manageable units. Database administrators can perform maintenance tasks, such as backups or schema updates, on individual shards without affecting the entire system. This modular approach reduces the risk of large-scale disruptions and simplifies the overall management of the database infrastructure~\parencite{db_sharding}. Of course, this also brings the risk that unnoticed bugs or bad configurations can spread more easily and make the work of debugging and repairing the databases more complicated. 

\subsection{Challenges and Considerations}
One of the most critical challenges is maintaining consistency across shards. In a distributed environment, ensuring that all shards reflect the most recent data state can become difficult, especially in case of network partitions or node failures. To try to counter this, techniques such as distributed transactions and eventual consistency models are often used, but they will again add additional complexity to the system~\parencite{skyline_joins}.

Another challenge is re-balancing shards as the data grows or usage patterns change. Over time, some shards may become "hot" (i.e., they handle a disproportionate amount of traffic in comparison to other shards), leading to performance bottlenecks. Re-balancing involves redistributing data across shards to ensure even load distribution, which can be a complex and resource-intensive process~\parencite{db_sharding_cloud}.

Lastly, sharding can complicate query processing and potentially have a negative impact on performance. Cross-shard queries, where data from multiple shards need to be executed in an efficient manner. While there are special approaches to optimize distributed queries such as \textit{Reference/Distributed Table Joins} or \textit{Remote Distributed Table Joins}, careful query planning and optimization are still required to minimize the impact~\parencite{db_sharding_joins}.

To conclude this section, it's important to repeat that database sharding allows for high scalability, fault tolerance and manageability, making it a valid approach for modern database infrastructure, but at the same time also introduces challenges such as consistency management, shard rebalancing, and complex query processing. At the end, most services will more likely simply increase their current database's compute and storage power - and worry about other approaches such as sharding only when they reach a limit with their current system~\parencite{db_sharding_newsql}.   
