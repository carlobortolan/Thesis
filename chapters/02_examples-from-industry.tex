% !TeX root = ../main.tex
\chapter{Examples from Industry}\label{chapter:examples}

This chapter gives a broad overview of the history of video streaming (see \autoref{subsection:video-streaming-types} for the definition of \textit{video streaming}) and then presents three aspects of video services in detail using examples from the well-known video streaming services: First a look at the system architecture of a streaming service, using the example of Twitch, then a look at how Netflix works together with \ac{ISPs} to deliver content as efficiently as possible and lastly a section on how YouTube handled different scalability challenges and developed its own video transcoding system.     

\section{History}
% START TODO
The video streaming industry has evolved dramatically over the past decades, transitioning from old-fashioned technologies to highly complex, scalable infrastructures capable of simultaneously delivering large amounts of content to hundreds of millions of users. The origins of video streaming can be traced back to the late 1990s and early 2000s, with the introduction of streaming technologies like RealNetworks' RealPlayer and Apple's QuickTime. These early platforms allowed users to stream audio and video content, although it was of low quality and had significant buffering issues due to limited bandwidth and server capacities.
%END TODO
Another important precursor to modern streaming was \ac{MBone}, a virtual network introduced in the early 1990s to support audio and video broadcasts over the internet, showing the potential behind internet-based multimedia transmission. Around the same time, web conferencing (often called webcast) became popular for video-chat tools, multiuser application-sharing and real-time polls. 
After the widespread adoption of Adobe Flash in the early 2000s, the interest in a single, unified streaming format started the development of a Flash-based streaming format, which was then used by Flash video players on most streaming sites.
% START TODO
The real breakthrough in video streaming came in 2005 with the launch of YouTube, which introduced a user-friendly platform for uploading, sharing and streaming videos online. 
% END TODO
YouTube's success not only showed the willingness of users to produce, upload and consume hours of original video content via the internet but also the potential for scalable video streaming infrastructure, increasing the demand for online video content. At the same time, Netflix's pivot from a mail-based DVD rental business to streaming in 2007 set another milestone in the industry. 

\noindent As internet speeds increased and cloud computing became more prevalent, the scalability of video streaming services improved (Netflix closed its last physical data center in mid-2016~\parencite{netflix_cloud}).
% START TODO
Additionally, the introduction of \ac{ABR} streaming allowed platforms to dynamically adjust video quality depending on the user's internet connection, reducing loading times and improving the overall viewing experience~\parencite{abr}. Meanwhile, using a \ac{CDN} became increasingly important for caching video data closer to users, thereby reducing latency and server load~\parencite{cdn_basic}.
% END TODO

\section{Example: Twitch}
Twitch launched in 2011 as a spin-off of the general interest streaming platform Justin.tv. Since then, it has become synonymous with live streaming, particularly within the gaming community and is still growing rapidly across different categories. 

One of Twitch's most significant challenges is managing vast amounts of user-generated content in real-time and scale with high fluctuations in viewer numbers, particularly during major events like e-sports competitions.
Twitch's architecture is highly complex and needs to support millions of concurrent video streams, real-time interactions through chat and extensive data processing such as predictive modeling for personalized recommendations, spam detection for chat messages and targeted campaigns based on in-app user behavior~\parencite{twitch_analytics}. In the following, there is a detailed breakdown of the key components of its architecture (see also \autoref{fig:twitch-architecture}):

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\linewidth]{images/TwitchArchitectureNew.png}
    \caption[System Architecture of Twitch]{System Architecture of Twitch}\label{fig:twitch-architecture}
\end{figure}

\begin{enumerate}
    \item \textbf{Video System}
    
    The video system begins with the \textbf{video ingest} process, where \ac{RTMP} live streams are received from streamers. Once the video stream is ingested, it is transported to the \textbf{transcode sub-system}. 
    % START TODO
    This sub-system, implemented in a combination of C/C++ and Go, transcodes the incoming \ac{RTMP} stream into multiple \ac{HLS} streams to allow viewers to switch between different resolutions when watching a stream.
    After transcoding, the streams are distributed through Twitch's global \textbf{Distribution and Edge} network consisting of multiple \ac{POPs}. The \ac{POPs} cache the \ac{HLS} streams and deliver them to users from geographically optimal locations, minimizing latency and buffering. The distribution system, also largely written in Go, is designed to scale massively to ensure high availability even during peak usage times.
    % END TODO
    Additionally, Twitch archives all live streams through its \textbf{\ac{VOD} system}, making content available for later viewing, either immediately after the live broadcast or as part of a long-term archive~\parencite{twitch_engineering}.
    
    \item \textbf{Chat}

    The chat system is a real-time distributed system, primarily written in Go, and designed to handle real-time interaction between viewers and streamers. The \textbf{Edge} component of the chat system is responsible for receiving and distributing messages between clients and backend services.
    
    Next, the \textbf{Pubsub} subsystem is used for the internal distribution of chat messages across various Edge nodes. Together, they create a hierarchical message distribution system capable of executing massive fanout to guarantee that all users in a chat room receive messages quickly and reliably. The \textbf{Clue} component handles the application of business logic to these chat interactions. For example, it authorizes the user's message by checking if they are banned from a channel, whether they are a subscriber, or if they are showing abusive behavior. Clue achieves this by accepting messages forwarded by the Edge nodes and then aggregating data from various databases, internal \ac{API}s and caches. Using this data, the viewers' messages are evaluated against existing rules in real time. Finally, the \textbf{Room} component manages the viewer list for each chat room. It aggregates, stores and queries membership data across all Edge nodes to provide accurate and up-to-date viewer lists, which are crucial for both moderation and user interaction~\parencite{twitch_chat}.

    \item \textbf{Web \ac{API}s and Data}

    Twitch’s platform also includes a set of web \ac{API}s and data analysis services for various purposes, e.g., from user profile management to stream discovery. These \textbf{Web \ac{API}s} are built using a combination of Ruby on Rails, Go and other open-source frameworks, designed to handle high request volumes, with Twitch's services processing over 50,000 \ac{API} requests per second on average. These \ac{API}s allow users to manage their profiles, customize subscriptions and interact with other services on the platform~\parencite{twitch_engineering}.

    Additionally, Twitch also has various microservices for specific use cases, such as \textbf{Search and Discovery Services} to help users find streams and content that match their interests or \textbf{Revenue Systems} that manage all aspects of advertising and subscriptions, ensuring that revenue is accurately tracked and distributed to partners~\parencite{twitch_engineering}. 
    
    \break \item \textbf{Web and Client Applications}
    
    Twitch’s \textbf{Desktop Web Application} began as a vanilla Rails application but has developed into an Ember.js application. For mobile users, Twitch offers \textbf{Native Apps} on iOS and Android as well as \textbf{Console Applications} for major gaming systems, including Xbox One, Xbox 360 and PlayStation 4~\parencite{twitch_engineering}.

    \item \textbf{Data Science Infrastructure}

    Next to its operating infrastructure, Twitch’s data science infrastructure plays an important role in optimizing the platform, improving user experiences and driving business decisions. At the core of this infrastructure is the \textbf{Data Pipeline}, which is responsible for collecting, cleaning and loading over a billion events per day into Twitch’s data warehouse~\parencite{twitch_engineering}. 

    The platform also uses so-called \textbf{Streaming Aggregators} to summarize key metrics in near real-time and provide broadcasters with direct feedback on their stream performance~\parencite{twitch_engineering}.

    \item \textbf{Tools and Operational Infrastructure}

    \ac{QA} is critical, with Twitch utilizing both \textbf{Automated Testing Frameworks} such as Jenkins to allow for continuous integration and testing to maintain high code quality across all services.

    In addition to that, there are also several \textbf{Deployment and Rollback Tools}, as well as \textbf{Monitoring and Alerting Systems}, including \href{https://ganglia.info}{Ganglia}\footnote{\url{https://ganglia.info}}, \href{https://graphiteapp.org}{Graphite}\footnote{\url{https://graphiteapp.org}} and \href{https://www.nagios.org}{Nagios}\footnote{\url{https://www.nagios.org}} that monitor the status and performance of the infrastructure, providing real-time alerts and insights that help engineers quickly identify and prevent problems~\parencite{twitch_engineering}.

    Lastly, Twitch’s \textbf{Network Infrastructure} mostly operates on bare-metal \ac{POPs} worldwide while an increasing number of services are being migrated to \ac{AWS}, which helps reduce operational overhead while benefiting from the on-demand scalability and flexibility of cloud services.

\end{enumerate}

\section{Example: Netflix}


\subsection{Overview of Netflix Open Connect}

Netflix Open Connect was developed in 2011 and officially launched in 2012 as a response to the rapidly increasing scale of Netflix's streaming service~\parencite{netflix_functionality}. Before this, Netflix relied on third-party \ac{CDN}s to deliver content to its users. However, as Netflix's global internet traffic grew, it became evident that a custom-built \ac{CDN} could provide more efficiency and better performance adapted to Netflix's specific needs.
The core of Netflix Open Connect is its global network of \ac{OCAs}, which consists of more than 8,000 specialized servers located in over 1,000 locations around the world~\parencite{netflix_open_connect}. These \ac{OCAs} are placed strategically in \ac{ISPs}' data centers to allow Netflix to deliver content directly to the users without relying heavily on the general infrastructure of the internet.
When a user signs into Netflix or performs other account-related actions, the user's client makes a request to the central server(s), which can potentially be far away from the user's location. However, when a video is requested, the request is sent to one of the nearby \ac{OCAs}, which then serves the large video files directly to the user. Hence, this reduces latency and minimizes the amount of traffic, as the cached video data only needs to travel from the OCA to the user.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\linewidth]{images/NetflixISP.png}
    \caption[Netflix Open Connect Network as of 2016]{Netflix Open Connect Network as of 2016~\parencite{netflix_open_connect}}\label{fig:netflix-isp}
\end{figure}

\subsection{Content Delivery Mechanism}

Netflix's content delivery strategy involves pre-positioning content on \ac{OCAs} before users request it. This is achieved through advanced algorithms that analyze and predict popular content to make sure that the most likely to be watched content is already stored close to where it will be consumed~\parencite{netflix_cloud}. For example, in regions with limited internet capacity, such as Australia, Netflix minimizes the use of undersea cables by pre-loading content onto \ac{OCAs} during off-peak hours, reducing the need for real-time content transmission over these expensive and bandwidth-limited connections.

\subsection{Performance Optimization and Energy Efficiency}

Over the years, Netflix has continued to optimize its \ac{OCAs}, increasing their efficiency by an order of magnitude since the program's creation. For instance, the throughput of a single server has increased from 8 Gbps in 2012 to over 100 Gbps\footnote{For FHD (1080p) resolution, which requires a download speed of around 5 Mbps (see \url{https://help.netflix.com/node/306}), 100 Gbps would support 20,000 simultaneous streams.} in 2017 of encrypted traffic from a single OCA~\parencite{netflix_content_serving}, largely due to improvements in both hardware and software~\parencite{netflix_open_connect}. These advancements have also led to smaller and more power-efficient \ac{OCAs}.

\section{Example: YouTube}

YouTube, launched in 2005, is the world's largest video-sharing platform, with more than 500 hours of videos uploaded every minute and approximately 694,000 hours of video content streamed per minute~\parencite{youtube_stats}.

\subsection{General Architecture}

The following system design model (see \autoref{fig:youtube-system-design}) will not be explained in detail and is meant to help understand and visualize components referenced in this section.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=350pt]{images/YoutubeSystemArchitecture.png}
    \caption[System Architecture of YouTube]{System Architecture of YouTube}\label{fig:youtube-system-design}
\end{figure}

\noindent YouTube's infrastructure is built on Google's global network of data centers and \ac{CDN}s: The most popular content for a certain region is cached in global \ac{CDN}s that make sure that there are multiple copies of a video to guarantee quality of service, reliability and closeness to the user while the other videos are stored on separate servers in various locations, mainly in the US~\parencite{youtube_architecture_2}. The platform uses advanced video coding formats, such as \href{https://developers.google.com/media/vp9}{VP9}\footnote{VP9 is a video coding format developed by Google and published in 2013 to compete with H.265, designed to handle high resolutions up to 65536×65536 pixels and mainly used by YouTube. It works by compressing video content using block-based transformation, where the image is divided into coding units of 64×64 pixels, which are further subdivided into smaller blocks based on the content complexity. A draft of the specification can be found at \url{https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf}} to optimize bandwidth usage while maintaining high video quality, even at lower bitrates~\parencite{youtube_vpu}. 
In recent years, YouTube has also expanded its feature offering to include live streaming and 360-degree videos~\parencite{youtube_live}.




\subsection{Scalability Challenges}\label{subsection:yt-scalability-challenges}

While there is not much information on current scalability issues published by YouTube, there are several records of problems the YouTube team faced up to 2012. These challenges included managing rapid and unpredictable user growth with limited resources, as there was insufficient budget to maintain excess machines to handle such growth. At the same time, the introduction of compute-heavy features, such as social graphs and algorithms for recommender systems, did not make it easier to find resources for video processing and storage. The team also pushed the limits of the hardware and software of that time, encountering limitations like performance bottlenecks in database structures before database partitioning (more details in \autoref{section:db-sharding}) ~\parencite{youtube_challenges}. As the platform continued to evolve, identifying these bottlenecks became more difficult, as they often were not due to an issue by YouTube itself, but rather by limitations of other libraries and third-party software they used~\parencite{youtube_challenges_2}.

\subsection{Video Acceleration at Scale}

With Moore's Law slowing down, specialized hardware accelerators optimized for large-scale video transcoding are needed to meet the demand for video processing. 
In 2021, the YouTube team presented an accelerator called the \ac{VCU} and showed that by scaling it within Google data centers, it achieved 20-33x improved efficiency\footnote{Measured by performance-per-TCO (total cost of ownership) with values above 1.0x indicating better performance or cost-efficiency compared to the current non-accelerated reference system (Skylake).} compared to the previous well-tuned non-accelerated systems (see \autoref{tab:youtube-vcu})~\parencite{youtube_infrastructure}.

\begin{table}[h!]
\centering
\caption{Throughput and Performance Comparison of Systems~\parencite{youtube_vpu}}\label{tab:youtube-vcu}
\begin{tabular}{|l|cc|cc|}
\hline
\textbf{System}      & \multicolumn{2}{c|}{\textbf{Throughput [Mpix/s]}} & \multicolumn{2}{c|}{\textbf{Perf/TCO}}    \\ \hline
            & \multicolumn{1}{c|}{H.264}       & VP9       & \multicolumn{1}{c|}{H.264} & VP9 \\ \hline
Skylake     & \multicolumn{1}{c|}{714}            & 154           & \multicolumn{1}{c|}{1.0x}      & 1.0x     \\ \hline
4xNvidia T4 & \multicolumn{1}{c|}{2,484}          & --            & \multicolumn{1}{c|}{1.5x}      & --       \\ \hline
8xVCU       & \multicolumn{1}{c|}{5,973}          & 6,122         & \multicolumn{1}{c|}{4.4x}      & \textbf{20.8x}    \\ \hline
20xVCU      & \multicolumn{1}{c|}{14,932}         & 15,306        & \multicolumn{1}{c|}{7.0x}      & \textbf{33.3x}    \\ \hline
\end{tabular}
\end{table}

The core component of the \ac{VCU} system is the encoder core, which serves as a special hardware component acting as an accelerator built for large distributed clusters. 
% It works in three stages: 1. \href{https://en.wikipedia.org/wiki/Macroblock}{\textit{Block-based video encoding}}\footnote{\url{https://en.wikipedia.org/wiki/Macroblock}} (most memory and bandwidth intensive stage), 2. \href{https://en.wikipedia.org/wiki/Entropy_coding}{\textit{entropy encoding}}\footnote{\url{https://en.wikipedia.org/wiki/Entropy_coding}} and 3. \textit{\href{https://www.kernel.org/doc/html/latest/gpu/afbc.html}{loop filtering and lossless frame buffer compression}}\footnote{\url{https://www.kernel.org/doc/html/latest/gpu/afbc.html}}. 
While the \ac{VCU} design optimizes for throughput and system balance, allowing it to adapt to changing workloads and infrastructure demands, its major challenges are handling multiple resolutions and formats and ensuring system reliability in a large-scale environment~\parencite{youtube_vpu}.
Also, the \ac{VCU} integrates multiple encoder cores and a memory system optimized for data center workloads, which allows it to be adjusted dynamically depending on network bandwidth, storage and live/\ac{VOD} demand workloads. This hardware-software co-design approach helps efficiently transcode uploaded videos while being flexible for future workloads. The system is highly parallelized, allowing for efficient handling of multi-output transcoding where a single input video is processed into multiple resolutions and formats at the same time~\parencite{youtube_vpu}.

The \ac{VCU} system enables otherwise infeasible VP9 compression at scale, caused by VP9's larger block sizes and limited hardware support. This results in new use cases related to live streaming (e.g., using VP9 to encode many short (2-second) segments in parallel to increase overall throughput). \blockquote{\textit{As a concrete example, a 2-second 1080p chunk could be encoded in 10 seconds, the encoding system would transcode 5-6 chunks concurrently to achieve the needed throughput of a 1 video-sec/second}}~\parencite{youtube_vpu}.
Currently, this system is mainly used by YouTube for a wide range of video workloads, including video sharing, live streaming and cloud gaming. 